{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sindhya456/Emotions-Recognition/blob/master/THIS_ONE_of_Lumo_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # select dataset.zip"
      ],
      "metadata": {
        "id": "zP2wuDDPhhaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q rl_datasets.zip -d /content/"
      ],
      "metadata": {
        "id": "w8BRGSFBhkFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash\n"
      ],
      "metadata": {
        "id": "Yn8_LNXQh_i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX_ty1L5fP7v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "assembled_dir = \"/content/rl_dataset/assembled\"\n",
        "parts_dir = \"/content/rl_dataset/parts\"\n",
        "\n",
        "pairs = []\n",
        "\n",
        "# Get all parts files indexed by object name (ignore extension)\n",
        "parts_map = {}\n",
        "for fname in os.listdir(parts_dir):\n",
        "    obj_name = fname.replace(\"parts_\", \"\").split(\".\")[0]  # remove prefix + extension\n",
        "    parts_map[obj_name] = os.path.join(parts_dir, fname)\n",
        "\n",
        "# Try matching assembled images to parts\n",
        "for fname in os.listdir(assembled_dir):\n",
        "    obj_name = fname.replace(\"assembled_\", \"\").split(\".\")[0]  # remove prefix + extension\n",
        "    assembled_path = os.path.join(assembled_dir, fname)\n",
        "\n",
        "    if obj_name in parts_map:\n",
        "        pairs.append({\"input\": assembled_path, \"output\": parts_map[obj_name]})\n",
        "    else:\n",
        "        print(f\"⚠️ No match found for {obj_name}\")\n",
        "\n",
        "df = pd.DataFrame(pairs)\n",
        "df.to_csv(\"rl_dataset.csv\", index=False)\n",
        "print(f\"✅ Final dataset size: {len(df)} pairs\")\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (assuming rl_dataset.csv already created)\n",
        "df = pd.read_csv(\"rl_dataset.csv\")\n",
        "\n",
        "def show_dataset_samples(df, n=5):\n",
        "    \"\"\"Show n random assembled ↔ parts pairs from the dataset.\"\"\"\n",
        "    sample = df.sample(n)\n",
        "\n",
        "    plt.figure(figsize=(10, 2*n))\n",
        "\n",
        "    for i, row in enumerate(sample.itertuples(), 1):\n",
        "        # Show assembled image (input)\n",
        "        assembled_img = Image.open(row.input)\n",
        "        plt.subplot(n, 2, 2*i-1)\n",
        "        plt.imshow(assembled_img)\n",
        "        plt.title(\"Assembled\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Show parts image (output)\n",
        "        parts_img = Image.open(row.output)\n",
        "        plt.subplot(n, 2, 2*i)\n",
        "        plt.imshow(parts_img)\n",
        "        plt.title(\"Parts\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: visualize 5 random pairs\n",
        "show_dataset_samples(df, n=5)\n"
      ],
      "metadata": {
        "id": "9DPZH7WsjKBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image segmentation Model"
      ],
      "metadata": {
        "id": "Thwx6vtdsAbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Imports\n",
        "# ==========================\n",
        "%matplotlib inline\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ==========================\n",
        "# User settings\n",
        "# ==========================\n",
        "csv_file = \"/content/rl_dataset.csv\"    # CSV with columns \"input\",\"output\"\n",
        "test_image = \"/content/table.png\"       # single test image\n",
        "epochs = 20\n",
        "batch_size = 8\n",
        "img_size = 128\n",
        "lr = 1e-3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_parts = 32  # expected number of semantic part classes\n",
        "\n",
        "# ==========================\n",
        "# Mask utilities\n",
        "# ==========================\n",
        "def quantize_mask(mask_pil, step=16):\n",
        "    arr = np.array(mask_pil.convert(\"RGB\"))\n",
        "    arr = (arr // step) * step\n",
        "    return Image.fromarray(arr.astype(np.uint8))\n",
        "\n",
        "def build_color_palette_robust(csv_path, mask_col=\"output\", size=(128,128), n_parts=32):\n",
        "    \"\"\"\n",
        "    Build a color2idx mapping using k-means clustering on all mask pixels.\n",
        "    n_parts: expected number of semantic parts (output classes)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    all_pixels = []\n",
        "\n",
        "    for mask_path in df[mask_col]:\n",
        "        mask = Image.open(mask_path).convert(\"RGB\").resize(size, Image.NEAREST)\n",
        "        arr = np.array(mask).reshape(-1,3)\n",
        "        all_pixels.append(arr)\n",
        "\n",
        "    all_pixels = np.vstack(all_pixels)\n",
        "\n",
        "    # KMeans clustering to reduce to n_parts\n",
        "    kmeans = KMeans(n_clusters=n_parts, random_state=42, n_init=10)\n",
        "    kmeans.fit(all_pixels)\n",
        "    colors = [tuple(map(int, c)) for c in kmeans.cluster_centers_]\n",
        "\n",
        "    color2idx = {c:i for i,c in enumerate(colors)}\n",
        "    palette = colors\n",
        "    print(f\"Built robust palette with {len(palette)} classes.\")\n",
        "    return color2idx, palette\n",
        "\n",
        "def color_mask_to_label(mask_pil, color2idx, size=(128,128)):\n",
        "    mask = mask_pil.resize(size, resample=Image.NEAREST)\n",
        "    arr = np.array(mask).reshape(-1,3)\n",
        "    # assign each pixel to nearest palette color\n",
        "    pixels = arr.astype(np.float32)\n",
        "    palette_arr = np.array(list(color2idx.keys())).astype(np.float32)\n",
        "    # compute distances\n",
        "    dists = np.linalg.norm(pixels[:,None,:]-palette_arr[None,:,:], axis=2)\n",
        "    idxs = dists.argmin(axis=1)\n",
        "    return torch.from_numpy(idxs.reshape(size[1], size[0])).long()\n",
        "\n",
        "def label_to_color_mask(label_tensor, palette):\n",
        "    if isinstance(label_tensor, torch.Tensor):\n",
        "        label = label_tensor.cpu().numpy()\n",
        "    else:\n",
        "        label = label_tensor\n",
        "    h,w = label.shape\n",
        "    out = np.zeros((h,w,3), dtype=np.uint8)\n",
        "    for idx, col in enumerate(palette):\n",
        "        mask = (label == idx)\n",
        "        out[mask] = col\n",
        "    return Image.fromarray(out)\n",
        "\n",
        "# ==========================\n",
        "# Dataset\n",
        "# ==========================\n",
        "class SegDataset(Dataset):\n",
        "    def __init__(self, csv_file, color2idx, size=(128,128), input_col=\"input\", mask_col=\"output\"):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.color2idx = color2idx\n",
        "        self.size = size\n",
        "        self.input_col = input_col\n",
        "        self.mask_col = mask_col\n",
        "\n",
        "        self.input_transform = transforms.Compose([\n",
        "            transforms.Resize(size, interpolation=Image.BILINEAR),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.input_col]).convert(\"RGB\")\n",
        "        mask = Image.open(row[self.mask_col]).convert(\"RGB\")\n",
        "        img = self.input_transform(img)\n",
        "        mask_lbl = color_mask_to_label(mask, self.color2idx, size=self.size)\n",
        "        return img, mask_lbl\n",
        "\n",
        "# ==========================\n",
        "# Dice & IoU functions\n",
        "# ==========================\n",
        "def dice_loss_logits(logits, targets, eps=1e-6):\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    B, C, H, W = probs.shape\n",
        "    targets_onehot = F.one_hot(targets.long(), num_classes=C).permute(0,3,1,2).float()\n",
        "    inter = (probs * targets_onehot).sum(dim=[2,3])\n",
        "    unions = (probs + targets_onehot).sum(dim=[2,3])\n",
        "    dice = (2*inter + eps) / (unions + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "def mean_iou(logits, targets, num_classes):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_mask = (preds == cls)\n",
        "        true_mask = (targets == cls)\n",
        "        inter = (pred_mask & true_mask).sum().item()\n",
        "        union = (pred_mask | true_mask).sum().item()\n",
        "        if union == 0:\n",
        "            continue\n",
        "        ious.append(inter / union)\n",
        "    if len(ious) == 0:\n",
        "        return 0.0\n",
        "    return float(np.mean(ious))\n",
        "\n",
        "# ==========================\n",
        "# U-Net model\n",
        "# ==========================\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, base=32, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.enc1 = DoubleConv(in_ch, base)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(base, base*2)\n",
        "        self.enc3 = DoubleConv(base*2, base*4)\n",
        "        self.enc4 = DoubleConv(base*4, base*8)\n",
        "        self.bottleneck = DoubleConv(base*8, base*16)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(base*16, base*8, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(base*16, base*8)\n",
        "        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(base*8, base*4)\n",
        "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(base*4, base*2)\n",
        "        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(base*2, base)\n",
        "\n",
        "        self.outc = nn.Conv2d(base, num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc1(x)\n",
        "        x2 = self.enc2(self.pool(x1))\n",
        "        x3 = self.enc3(self.pool(x2))\n",
        "        x4 = self.enc4(self.pool(x3))\n",
        "        b = self.bottleneck(self.pool(x4))\n",
        "\n",
        "        d4 = self.up4(b)\n",
        "        d4 = torch.cat([d4, x4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, x3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, x2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, x1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "        out = self.outc(d1)\n",
        "        return out\n",
        "\n",
        "# ==========================\n",
        "# Build palette & dataset\n",
        "# ==========================\n",
        "color2idx, palette = build_color_palette_robust(csv_file, mask_col=\"output\", size=(img_size,img_size), n_parts=n_parts)\n",
        "num_classes = len(palette)\n",
        "dataset = SegDataset(csv_file, color2idx, size=(img_size, img_size))\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ==========================\n",
        "# Model, optimizer, loss\n",
        "# ==========================\n",
        "model = UNet(in_ch=3, base=32, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================\n",
        "# Training loop\n",
        "# ==========================\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    steps = 0\n",
        "    for imgs, masks in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        logits = model(imgs)\n",
        "        loss_ce = ce_loss(logits, masks)\n",
        "        loss_dice = dice_loss_logits(logits, masks)\n",
        "        loss = loss_ce + loss_dice\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou += mean_iou(logits, masks, num_classes)\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | Loss: {total_loss/steps:.4f} | Mean IoU: {total_iou/steps:.4f}\")\n",
        "\n",
        "# ==========================\n",
        "# Test single image\n",
        "# ==========================\n",
        "def run_test(model, test_image_path, palette, size=(128,128)):\n",
        "    model.eval()\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    img = Image.open(test_image_path).convert(\"RGB\")\n",
        "    inp = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(inp)\n",
        "        pred = logits.argmax(dim=1)[0]\n",
        "    mask_out = label_to_color_mask(pred, palette)\n",
        "    # show input + predicted mask\n",
        "    fig, axs = plt.subplots(1,2, figsize=(6,3))\n",
        "    axs[0].imshow(img); axs[0].set_title(\"Input\")\n",
        "    axs[1].imshow(mask_out); axs[1].set_title(\"Predicted mask\")\n",
        "    for ax in axs: ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    # save output\n",
        "    mask_out.save(\"predicted_mask.png\")\n",
        "    print(\"Predicted mask saved as predicted_mask.png\")\n",
        "\n",
        "run_test(model, test_image, palette, size=(img_size,img_size))\n"
      ],
      "metadata": {
        "id": "B1eLubssnD91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main one ?"
      ],
      "metadata": {
        "id": "hgIT30_-CLja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Files in assembled folder:\")\n",
        "assembled_files = os.listdir(\"/content/rl_dataset/assembled\")\n",
        "for f in assembled_files[:5]:  # Show first 5\n",
        "    print(f\"  {f}\")\n",
        "\n",
        "print(\"\\nFiles in parts folder:\")\n",
        "parts_files = os.listdir(\"/content/rl_dataset/parts\")\n",
        "for f in parts_files[:5]:  # Show first 5\n",
        "    print(f\"  {f}\")"
      ],
      "metadata": {
        "id": "MnQ3dbpAGbVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get all files\n",
        "assembled_files = sorted(os.listdir(\"/content/rl_dataset/assembled\"))\n",
        "parts_files = sorted(os.listdir(\"/content/rl_dataset/parts\"))\n",
        "\n",
        "print(f\"Total assembled: {len(assembled_files)}\")\n",
        "print(f\"Total parts: {len(parts_files)}\")\n",
        "\n",
        "# Extract base names\n",
        "assembled_bases = set()\n",
        "for f in assembled_files:\n",
        "    base = f.replace('assembled_', '').replace('.png', '').replace('.jpg', '')\n",
        "    assembled_bases.add(base)\n",
        "\n",
        "parts_bases = set()\n",
        "for f in parts_files:\n",
        "    base = f.replace('parts_', '').replace('.png', '').replace('.jpg', '')\n",
        "    parts_bases.add(base)\n",
        "\n",
        "# Find matches\n",
        "matches = assembled_bases & parts_bases\n",
        "print(f\"\\nMatching items: {len(matches)}\")\n",
        "print(f\"Matches: {sorted(list(matches))[:10]}\")  # Show first 10\n",
        "\n",
        "# Find mismatches\n",
        "only_assembled = assembled_bases - parts_bases\n",
        "only_parts = parts_bases - assembled_bases\n",
        "\n",
        "print(f\"\\nOnly in assembled: {len(only_assembled)}\")\n",
        "print(f\"Examples: {list(only_assembled)[:5]}\")\n",
        "\n",
        "print(f\"\\nOnly in parts: {len(only_parts)}\")\n",
        "print(f\"Examples: {list(only_parts)[:5]}\")"
      ],
      "metadata": {
        "id": "tYJiA7DAGm5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class CADImageDataset(Dataset):\n",
        "    def __init__(self, input_dir, output_dir, transform=None):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.transform = transform\n",
        "        self.image_filenames = sorted(os.listdir(input_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assembled_filename = self.image_filenames[idx]\n",
        "\n",
        "        # Convert: assembled_coatstand.jpg/.png -> parts_coatstand.png\n",
        "        base_name = assembled_filename.replace('assembled_', '')\n",
        "        base_name = os.path.splitext(base_name)[0]\n",
        "        disassembled_filename = f'parts_{base_name}.png'\n",
        "\n",
        "        input_path = os.path.join(self.input_dir, assembled_filename)\n",
        "        output_path = os.path.join(self.output_dir, disassembled_filename)\n",
        "\n",
        "        input_img = Image.open(input_path).convert(\"L\")\n",
        "        output_img = Image.open(output_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            input_img = self.transform(input_img)\n",
        "            output_img = self.transform(output_img)\n",
        "\n",
        "        return input_img, output_img\n",
        "\n",
        "# -------------------------\n",
        "# Perceptual Loss\n",
        "# -------------------------\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, layers=[2, 7, 12]):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
        "        self.slices = nn.ModuleList([vgg[:l].eval() for l in layers])\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        if pred.shape[1] == 1:\n",
        "            pred = pred.repeat(1, 3, 1, 1)\n",
        "            target = target.repeat(1, 3, 1, 1)\n",
        "        loss = 0\n",
        "        for slice in self.slices:\n",
        "            loss += F.l1_loss(slice(pred), slice(target))\n",
        "        return loss\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Generator\n",
        "# -------------------------\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
        "        super(UNetBlock, self).__init__()\n",
        "        self.down = down\n",
        "        if down:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 4, 2, 1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        self.use_dropout = use_dropout\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, output_channels=1):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.down1 = UNetBlock(input_channels, 64)\n",
        "        self.down2 = UNetBlock(64, 128)\n",
        "        self.down3 = UNetBlock(128, 256)\n",
        "        self.down4 = UNetBlock(256, 512)\n",
        "        self.down5 = UNetBlock(512, 512)\n",
        "        self.down6 = UNetBlock(512, 512)\n",
        "        self.down7 = UNetBlock(512, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
        "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.up3 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
        "        self.up4 = UNetBlock(1024, 256, down=False)\n",
        "        self.up5 = UNetBlock(512, 128, down=False)\n",
        "        self.up6 = UNetBlock(256, 64, down=False)\n",
        "        self.final = nn.ConvTranspose2d(128, output_channels, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "\n",
        "        u1 = self.up1(d7)\n",
        "        u2 = self.up2(torch.cat([u1, d6], 1))\n",
        "        u3 = self.up3(torch.cat([u2, d5], 1))\n",
        "        u4 = self.up4(torch.cat([u3, d4], 1))\n",
        "        u5 = self.up5(torch.cat([u4, d3], 1))\n",
        "        u6 = self.up6(torch.cat([u5, d2], 1))\n",
        "        output = torch.tanh(self.final(torch.cat([u6, d1], 1)))\n",
        "        return output\n",
        "\n",
        "# -------------------------\n",
        "# Discriminator (DEFINE THIS FIRST)\n",
        "# -------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(input_channels * 2, 64, 4, 2, 1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, 2, 1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, 1, 1)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 1, 4, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.model(torch.cat([x, y], 1))\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Multi-Scale Discriminator (NOW THIS CAN USE Discriminator)\n",
        "# -------------------------\n",
        "class MultiScaleDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels=1):\n",
        "        super().__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            Discriminator(input_channels) for _ in range(3)\n",
        "        ])\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        results = []\n",
        "        for i, disc in enumerate(self.discriminators):\n",
        "            if i > 0:\n",
        "                x = self.downsample(x)\n",
        "                y = self.downsample(y)\n",
        "            results.append(disc(x, y))\n",
        "        return results\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Loss Functions\n",
        "# -------------------------\n",
        "def part_separation_loss(pred, min_separation=0.1):\n",
        "    \"\"\"Encourages distinct separated regions\"\"\"\n",
        "    edges_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
        "    edges_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
        "\n",
        "    weak_edge_penalty = torch.mean(torch.relu(min_separation - edges_x)) + \\\n",
        "                        torch.mean(torch.relu(min_separation - edges_y))\n",
        "\n",
        "    return weak_edge_penalty\n",
        "\n",
        "\n",
        "def gap_enforcement_loss(pred, target):\n",
        "    \"\"\"Penalize if there aren't enough 'background' pixels (gaps) between parts\"\"\"\n",
        "    background_mask = (pred < -0.5).float()\n",
        "    target_background = (target < -0.5).float()\n",
        "\n",
        "    gap_loss = F.mse_loss(torch.mean(background_mask), torch.mean(target_background))\n",
        "    return gap_loss\n",
        "\n",
        "\n",
        "def combined_loss(pred, target, lambda_ssim=0.2):\n",
        "    mse = F.mse_loss(pred, target)\n",
        "    # REMOVED TV loss - it was preventing sharp edges\n",
        "    ssim = torch.mean((1 - ((2 * pred * target + 0.01) / (pred ** 2 + target ** 2 + 0.01))))\n",
        "    return mse + lambda_ssim * ssim\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training Setup\n",
        "# -------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "input_dir = \"/content/rl_dataset/assembled\"\n",
        "output_dir = \"/content/rl_dataset/parts\"\n",
        "dataset = CADImageDataset(input_dir, output_dir, transform)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "G = GeneratorUNet().to(device)\n",
        "D = MultiScaleDiscriminator().to(device)  # Now this works!\n",
        "perceptual_loss_fn = PerceptualLoss().to(device)\n",
        "\n",
        "opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "epochs = 200\n",
        "scheduler_G = optim.lr_scheduler.StepLR(opt_G, step_size=30, gamma=0.5)\n",
        "scheduler_D = optim.lr_scheduler.StepLR(opt_D, step_size=30, gamma=0.5)\n",
        "\n",
        "# -------------------------\n",
        "# Training Loop\n",
        "# -------------------------\n",
        "for epoch in range(epochs):\n",
        "    for i, (input_img, output_img) in enumerate(loader):\n",
        "        input_img, output_img = input_img.to(device), output_img.to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_D.zero_grad()\n",
        "        fake_output = G(input_img)\n",
        "\n",
        "        real_preds = D(input_img, output_img)\n",
        "        fake_preds = D(input_img, fake_output.detach())\n",
        "\n",
        "        # Multi-scale discriminator loss\n",
        "        loss_D = 0\n",
        "        for real_pred, fake_pred in zip(real_preds, fake_preds):\n",
        "            loss_D += -torch.mean(torch.log(torch.sigmoid(real_pred) + 1e-8) +\n",
        "                                 torch.log(1 - torch.sigmoid(fake_pred) + 1e-8))\n",
        "\n",
        "        loss_D.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        opt_G.zero_grad()\n",
        "        fake_output = G(input_img)\n",
        "        fake_preds = D(input_img, fake_output)\n",
        "\n",
        "        # Multi-scale GAN loss\n",
        "        loss_GAN = 0\n",
        "        for fake_pred in fake_preds:\n",
        "            loss_GAN += -torch.mean(torch.log(torch.sigmoid(fake_pred) + 1e-8))\n",
        "\n",
        "        loss_recon = combined_loss(fake_output, output_img)\n",
        "        loss_perceptual = perceptual_loss_fn(fake_output, output_img)\n",
        "        loss_separation = part_separation_loss(fake_output)\n",
        "        loss_gaps = gap_enforcement_loss(fake_output, output_img)\n",
        "\n",
        "        # Adjusted weights: emphasize reconstruction and separation\n",
        "        loss_G = 2 * loss_GAN + 100 * loss_recon + 5.0 * loss_perceptual + \\\n",
        "                 5.0 * loss_separation + 3.0 * loss_gaps\n",
        "\n",
        "        loss_G.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "    # Step schedulers once per epoch\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  Loss_D: {loss_D.item():.4f}  Loss_G: {loss_G.item():.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Test Visualization\n",
        "# -------------------------\n",
        "test_input, test_output = next(iter(loader))\n",
        "test_input = test_input.to(device)\n",
        "with torch.no_grad():\n",
        "    pred = G(test_input)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(test_input[0].cpu().squeeze(), cmap=\"gray\")\n",
        "plt.title(\"Test Input\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(pred[0].cpu().squeeze(), cmap=\"gray\")\n",
        "plt.title(\"Predicted Disassembled\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ck1xI5ws5Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "segmenting parts images with masks-I tried to do opencv segmentation and the outputs were not that good so moving on to deep learning based segmentation\n",
        "\n",
        "after dl,switched to sam"
      ],
      "metadata": {
        "id": "0J0lxaY-oOME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- install dependencies ---\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python matplotlib\n",
        "\n",
        "import torch, cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "# --- load SAM model ---\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "\n",
        "# download checkpoint if not already\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "# --- load your test image ---\n",
        "img_path = \"/content/parts_coatstand.png\"\n",
        "image = cv2.imread(img_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# --- automatic mask generator ---\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,         # controls density of prompts, increase for more parts\n",
        "    pred_iou_thresh=0.88,       # confidence threshold\n",
        "    stability_score_thresh=0.90,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=200    # filter out tiny specks\n",
        ")\n",
        "\n",
        "masks = mask_generator.generate(image)\n",
        "print(f\"Generated {len(masks)} masks\")\n",
        "\n",
        "# --- visualize ---\n",
        "def show_anns(anns):\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0],\n",
        "                   sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_anns(masks)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "weBLPxqziBfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "geometric augmentation-NERF/diffusion"
      ],
      "metadata": {
        "id": "hR96SPShfV-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload parts_dataset.zip\n",
        "\n",
        "!unzip parts.zip -d parts\n",
        "!ls parts\n",
        "\n"
      ],
      "metadata": {
        "id": "mUhC8Y2avVlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying to get the sam generator to generate masks for all of the images--need more gpu!"
      ],
      "metadata": {
        "id": "h8m7-LScdyqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- install deps (if not already) ---\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python matplotlib\n",
        "\n",
        "import os, cv2, torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "\n",
        "# --- setup SAM ---\n",
        "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
        "model_type = \"vit_b\"\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=32,        # controls granularity (16=fast, 32=finer, 64=very fine but slow)\n",
        "    pred_iou_thresh=0.88,\n",
        "    stability_score_thresh=0.90,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2,\n",
        "    min_mask_region_area=200   # filter tiny blobs\n",
        ")\n",
        "\n",
        "# --- function to process one image ---\n",
        "def process_image(img_path, save_dir=\"sam_output\", visualize=False):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fname = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "    # load + convert to RGB\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"⚠️ Skipping {img_path} (not found)\")\n",
        "        return\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # generate masks\n",
        "    masks = mask_generator.generate(image)\n",
        "    print(f\"{fname}: {len(masks)} masks\")\n",
        "\n",
        "    # convert to stacked (N,H,W) array\n",
        "    H, W, _ = image.shape\n",
        "    stacked = np.zeros((len(masks), H, W), dtype=np.uint8)\n",
        "    for i, m in enumerate(masks):\n",
        "        stacked[i] = m[\"segmentation\"].astype(np.uint8)\n",
        "\n",
        "    # save tensor\n",
        "    np.save(os.path.join(save_dir, f\"{fname}_parts.npy\"), stacked)\n",
        "\n",
        "    # optional visualization\n",
        "    if visualize:\n",
        "        plt.figure(figsize=(8,8))\n",
        "        plt.imshow(image)\n",
        "        for m in masks:\n",
        "            mask = m[\"segmentation\"]\n",
        "            color = np.random.rand(3)\n",
        "            plt.imshow(np.dstack([mask*color[0], mask*color[1], mask*color[2]])*0.5, alpha=0.5)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"{fname}: {len(masks)} parts\")\n",
        "        plt.show()\n",
        "\n",
        "# --- process a folder of images ---\n",
        "input_dir = \"/content/rl/parts\"# change this to your folder\n",
        "for file in os.listdir(input_dir):\n",
        "    if file.lower().endswith((\".png\",\".jpg\",\".jpeg\")):\n",
        "        process_image(os.path.join(input_dir, file), visualize=True)\n"
      ],
      "metadata": {
        "id": "KZJ96RTfuVVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}